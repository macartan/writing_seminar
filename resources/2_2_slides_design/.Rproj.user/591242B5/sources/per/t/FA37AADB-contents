---
output:
  pdf_document: default
  html_document: default
---
# (PART) Models in Question  {-}


# Justifying models


***

We outline strategies to reduce reliance on unfounded beliefs about the probative value of clues.

***


```{r packagesused14, include = FALSE}
source("_packages_used.R")
do_diagnosis = FALSE
```



The approach we have  described to inference always involve updating beliefs given data. But to get off the ground researchers need to be able to state priors on all parameters. In many applications the problem of stating priors can be more fundamental than for many Bayesian applications for two reasons. First the  beliefs are beliefs over the distribution of individual level effects and not just the beliefs over average effects. This puts us up against the fundamental problem of causal inference (Holland cite, Dawid cite FLAG). Second, the beliefs can do a lot of work---especially in small $n$ applications. Indeed for the the process tracing described chapters 6 and 7 [FLAG ADD REFS] the inferences are little more than conditional applications of a model.      

We see two broad responses to this problem.

One is *emphasize the contingent nature of claims*. As we outlined in Chapter 4, some causal models might reasonably reflect actual beliefs about the world---for example one might,  be convinced that a treatment was randomly assigned, that there is no interference, and that units are independently sampled from a distribution of types. All of these beliefs may be unwise, of course. But if held, then the simple $X \rightarrow Y$ DAG in chapter 4 (FIGURE REF) is more of a representation of beliefs about the world  than it is a model of the world, in the sense of a simplified representation.^[Even in this simple case there are ways in which the representation is a model, not least the coding of events as a variable invovlves a form of modelling.] But as we noted in Chapter 4, for an even modestly more complex situation, it seems inevitable that the model being used is unquetionanly a model and hard to think of as a faithful summary of beliefs. 

Recognizing in this way that we are generally dealing with models results in a useful reposing of the question: the question becomes not whether the assumptions are correct but whether the model is useful for some purpose [@clarke2012model]. That is the subject of Chapter 15.

Here we focus on more positive steps that might be taken to underpin a model. We show how  *the type of approach used in Chapters 8 and 9 can be used to justify a process tracing model on the basis of a mixed methods model*. These applications presuppose knowledge of a DAG however. In a sense, they simply push the question down a level. There are two further responses to this concern. One is to try to generate the DAG itself from data or a combination of data and theory. We discuss this approach here.  Another is to assess the importance of DAG assumptions -- which we address in Chapter 15.

## Justifying the classic process tracing tests

**The possibility of justification.**

Recall the four classical "qualitative tests" described by @collier2011understanding and drawing on @Van-Evera:1997 : are "smoking gun" tests, "hoop" tests, "doubly decisive" tests, and "straw-in-the-wind" tests. A hoop test is one which, if failed, bodes especially badly for a claim; a smoking gun test is one that bodes well for a hypothesis if passed; a doubly decisive test is strongly conclusive no matter what is found, and a straw-in-the-wind test is suggestive, though not conclusive, either way.

In some treatments (such as @humphreys2015mixing) formalization involves specifying a prior that a hypothesis is true and an independent set of beliefs about the probability of seeing some data if the hypothesis is true and if it is false. Then updating proceeds using Bayes' rule. 

This simple approach suffers from two related weaknesses however: first, there is no good reason to expect these probabilities to be independent; second, there is nothing in the set-up to indicate how beliefs around the probative value of clues can be established or justified. 

Both of these problems are resolvable if the problem is articulated using fully specified causal models.

We illustrate first by using an idealized example to show that a case level "doubly decisive" test can be justified by population data from factorial designs.

Say we have from oberving just $X$ and $Y$ that $\Pr(Y=1|X=1) = \Pr(Y=1|X=0) = .5$. Here we have an average treatment effect of 0. We are interested in a particular case with $X=1, Y=1$ and specifically whether  $X=1$ caused $Y=1$ in that case. The marginal information so far is consistent with a world in which $X$ never affects $Y$ and one in which $X$ always affects $Y$ (sometimes negatively, sometimes positively). 

Say now that we had data on $K$ and found (a) that $K=1$ arises with 50% probabilities, an (b) that the marginal distributions of $Y$ given $X$ and $K$ are as follows:

* $\Pr(Y=1|X=0, K = 0) = 1$
* $\Pr(Y=1|X=1, K = 0) = .5$
* $\Pr(Y=1|X=0, K = 1) = 0$
* $\Pr(Y=1|X=1, K = 1) = .5$

In that case we see that in cases in which $K=1$, $X=0$ is a necessary condition for $Y=1$. So if $K=1$ then certainly $X=1$ caused $Y=1$ (since, in that case, were $X$ zero then certainly $Y$ would be 0.) On the other hand were $K=0$ then $X=0$ woudl be a sufficient condition for $Y=1$, which means that in this case $X=1$ most certainly did *not* cause $Y=1$.  We have then that if $K=1$ then  certainly $X=1$ caused $Y=1$ whereas if  $K=0$ then  certainly $X=1$ did not cause $Y=1$
 
This argument demonstrates that it is possible to justify a doubly decisive test on the basis of experimental data---provided your case can be considered exchangeable with cases in the experimental data.  

* $\Pr(Y=1|X=0, K = 0) = 1$
* $\Pr(Y=1|X=1, K = 0) = .5$
* $\Pr(Y=1|X=0, K = 1) = 0$
* $\Pr(Y=1|X=1, K = 1) = .5$

| Test                 | Doubly decisive | Hoop | Smoking gun |  Straw in the wind | 
|----------------------|-----------------|------|-------------|--------------------|
|$\Pr(K = 1)$          | .5              | .9   | .1          |   .5               |
|$\Pr(Y=1|X=0, K = 0)$ | 1               |  1   | 4/9         |  .6                |
|$\Pr(Y=1|X=1, K = 0)$ | .5              | .5   | .5          |  .5                |
|$\Pr(Y=1|X=0, K = 1)$ | 0               | 4/9  |  0          |  .4                |
|$\Pr(Y=1|X=1, K = 1)$ | .5              | .5   | .5          |  .5                |
|$\Pr(X \text{  causes }Y|K=1)$ | 1        |       |   1           |               |
|$\Pr(X \text{  causes }Y|K=0)$ | 0        | 0     |             |                |

FLAG: PUT BOUNDS INEMPTY CELLS

**More practiaclly....*

In the table above some entries were given as ranges. This reflects that fact that unless we are at edge cases the estimand here is not identified even with infinite  experimental data. In practice we expect never to be at these edges. Even still, the procedures given in Chapters XX above let us form posteriors over inferences with finite data. 

For the illustration we first make use of a function that generates data from a model with a constrained set of types for $Y$ and a given prior distribution over clue $K$.

```{r}
van_evera_data <- function(y_types, k_types)
  
  make_model("X -> Y <- K") %>%
  
  set_restrictions(labels = list(Y = y_types), keep = TRUE) %>%
  
  set_parameters(param_type = "define", node = "K", parameters = c(1 - k_types, k_types)) %>%
  
  make_data(n = 1000)
```

We then use a function that draws inferences, given different values of a clue $K$, from a model that has been updated using available data. Note that the model that is updated has no constraints on $Y$, has flat beliefs over the distribution of $K$, and imposes no assumption that $K$ is informative for how $Y$ reacts to $X$. 

```{r}
van_evera_inference <- function(data)
  
  make_model("X -> Y <- K") %>%
  
  update_model(data = data) %>%  
  
  query_model(query = "Y[X=1] > Y[X=0]", 
              given = c(TRUE, "K==0", "K==1"),
              using = "posteriors")
```

We can now generate posterior beliefs, given $K$, for different types of tests where the tests are now justified by different types of data, coupled with a common prior causal model.
  
Results:

```{r, eval = FALSE}
doubly_decisive <- van_evera_data("0001", .5) %>% van_evera_inference

hoop            <- van_evera_data(c("0001", "0101"), .9) %>% van_evera_inference

smoking_gun     <- van_evera_data(c("0001", "0011"), .1) %>% van_evera_inference

straw_in_wind   <- van_evera_data(c("0001", "0101", "0011"), .5) %>% van_evera_inference
```



```{r, echo = FALSE}
if(do_diagnosis){
doubly_decisive <- van_evera_data("0001", .5) %>% van_evera_inference

hoop            <- van_evera_data(c("0001", "0101"), .9) %>% van_evera_inference

smoking_gun     <- van_evera_data(c("0001", "0011"), .1) %>% van_evera_inference

straw_in_wind   <- van_evera_data(c("0001", "0101", "0011"), .5) %>% van_evera_inference

write_rds(list(doubly_decisive, hoop, smoking_gun, straw_in_wind), "saved/ch14_vanevera.rds")
}

tests <- read_rds("saved/ch14_vanevera.rds")

tests[[1]] %>% kable(caption= "Doubly decisive test")
tests[[2]] %>%  kable(caption= "Hoop test")
tests[[3]] %>% kable(caption= "Smoking gun test")
tests[[4]] %>% kable(caption= "Straw in the wind test")

```


We see that these tests all behave as expected. Importantly, however, the approach to thinking about the tests is quite different to that described in @collier2011understanding or @humphreys2015mixing. Rather than having a belief about the probative value of a clue, and a prior over a hypothesis, inferences are drawn directly from a causal model that relates a clue to possible causal effects. Critically, with this approach,  the inferences made from observing clues can be justified by reference to a more fundamental,  agnostic model, that has been updated in light of data. The updated model yields both a prior over the proposition, belief about probative values, and guidance for what conclusions to draw given knowledge of $K$. 


## Nothing from nothing

We now share a more negative result. Say you had access to large amounts of observational data on $X$, $Y$ and $M$, but beyond knowing the temporal order of $X, Y, M$, you do now know the graph. Can you figure out whether $M$ is informative for $X$ causes $Y$ from this data?

```{r}

agnostic_data <- make_model("X->M->Y") %>% 
  set_parameters(node = c("M", "Y"), parameters = list(c(.1, 0, .8, .1),
                                                       c(.1, 0, .8, .1))) %>%
  simulate_data(n = 1000)

if(do_diagnosis){
  
agnostic_XMY <- make_model("X->M->Y<-X; X<->M; X<->Y; M <->Y")%>%
  update_model(agnostic_data)

write_rds(agnostic_XMY, "ch14_agnostic.rds")


agnostic_query <-  query_model(agnostic_XMY,
              query = "Y[X=1] > Y[X=0]", 
              given = c(TRUE, "M==0", "M==1"),
              using = "posteriors")

write_rds(agnostic_query, "ch14_agnostic_query.rds")


}

agnostic_query <- read_rds("ch14_agnostic_query.rds")

agnostic_query %>% kable(caption = "Conditional inferences from an update agnostic model given a true model in which $X$ causes $M$ and $M$ causes $Y$")
```

This example illustrates the Cartwight idea we pointed to in Chapter 2: no causes in, no causes out  @cartwright1994nature. 

<!-- ## Bounds on probative value -->

<!-- Classic treatments of process tracing make use of Causal Process Observations --- observations that are taken to be indicative of a particular causal process in operation. We introduced in Chapter 5 (as well as in FLAG CITE humphreysjacobs)  quantities such as $\phi_{b}$---the probability that $K=1$ given $X$ caused $Y$ and $X=Y=1$, or $\phi_{d}$-----the probability that $K=1$ given $X$ did not cause $Y$ and $X=Y=1$.  -->

<!-- These accounts do not guide much guidance however regarding where these quantities come from --- given that causal types are unobservable how can one justify a belief about the probability of some observation *given* a causal type. Is it even possible to justify such beliefs? -->

<!-- The grounded approach we described provides an answer to this puzzle. In short, knowledge of the structure of a causal model, together with data on exchangeable units, can be enough to place bounds on possible values of $\phi_{b}, \phi_{d}$.  -->

<!-- We illustrate the basic idea and then review some results in this area. -->

<!-- Imagine a fortunate situation in which (a) it is known that the true causal model has the form $X \rightarrow M \rightarrow Y$ and (b) we have a lot of experimental data on the conditional distribution of $M$ given $X$ and of $Y$ given $M$ for exchangeable units (meaning that we can treat our unit of interest as if it were a draw from this set).  -->

<!-- Let us define: -->

<!-- * $\tau_1 = \Pr(M=1 | X=1) - \Pr(M=1 | X=0)$ -->
<!-- * $\rho_1 = \Pr(M=1 | X=1) - \Pr(M=0 | X=0)$ -->
<!-- * $\tau_2 = \Pr(Y=1 | M=1) - \Pr(Y=1 | M=0)$ -->
<!-- * $\rho_2 = \Pr(Y=1 | M=1) - \Pr(Y=0 | M=0)$ -->

<!-- These are all quantities that can be calculated from the data. The $\tau$s are average treatment effects and the $\rho$s are indicators for how common the $Y=1$ outcome is. -->

<!-- We are interested in the probability of observing $M=1$ given $X=Y=1$: -->

<!-- $$\phi_{b1} = \frac{\lambda_{b}^K\lambda_{b}^Y}{\lambda_{b}^K\lambda_{b}^Y + \lambda_{a}^K\lambda_{a}^Y}$$ -->


<!-- Noting that $\tau_j = \lambda_{b_j} - \lambda_{a_j}$: -->

<!-- $$\phi_{b1} = \frac{\lambda_{b}^K\lambda_{b}^Y}{\lambda_{b}^K\lambda_{b}^Y + (\lambda_{b}^K-\tau_1)(\lambda_{b}^Y - \tau_2)}$$ -->
<!-- which we can see is decreasing in $\lambda_{b}^j$ (this may seem counterintuitive, but the reason is that with $\tau^j$ fixed, lower $\lambda_{b}^j$ also means lower $\lambda_{a}^j$ which means less ambiguity about *how* $X$ affects $Y$ (i.e. through positive or negative effects on $K$). -->

<!--  <!-- $$\phi_{1} = \frac{\lambda_{b}^Y}{2\lambda_{b}^Y -\tau_2 - \tau_1(\lambda_{b}^Y - \tau_2)/\lambda_{b}}^K$$ --> -->

<!-- The lowest permissible value of  $\lambda_{b_j}$  is $\tau_j$, yielding $\phi_{b1} = 1$.  -->

<!-- The highest value obtainable by $\lambda_{b_j}$ is when $\lambda_{a_j} = \frac{1-\tau_j+\rho_j}2$ and so $\lambda_{b_j} = \frac{1+\tau_j+\rho_j}2$.  -->

<!-- In this case: -->
<!-- $$\phi_{b1} = \frac{(1+\tau_1+\rho_1)(1+\tau_2+\rho_2)}{(1+\tau_1+\rho_1)(1+\tau_2+\rho_2) + (1-\tau_1+\rho_1)(1-\tau_2+\rho_2)}= \frac{(1+\tau_1+\rho_1)(1+\tau_2+\rho_2)}{2(1+\rho_1)(1+\rho_2) + 2\tau_1\tau}$$ -->

<!-- And so: -->

<!-- $$\frac{(1+\tau_1+\rho_1)(1+\tau_2+\rho_2)}{2(1+\rho_1)(1+\rho_2) + 2\tau_1\tau_2} \leq \phi_{b1} \leq 1$$ -->

<!-- These are the bounds on $\phi_{b1}$. We can calculate bounds on $\phi_{d1}$ in a similar way (though of course the bounds on  $\phi_{b1}$ and $\phi_{d1}$ are not independent).  -->


<!-- $$\phi_{d1} = \frac{\lambda_{b}^K\lambda_{d}^Y}{(\lambda_{a}^K + \lambda_{b}^K + \lambda_{c}^K)\lambda_{d}^Y+ \lambda_{c}^K\lambda_{a}^Y}$$ -->

<!-- Figure \@ref(fig:probval1) illustrates how "smoking gun" and "hoop" tests might each be justified with knowledge of $\tau_j, \rho_j$.  -->

<!-- ```{r phis85, echo = FALSE, include = FALSE} -->

<!-- make_phis <- function(model = make_model("X -> M -> Y"),  -->
<!--                         par = c(.5, .5, .3, 0, .6, .1, .1, 0, .6, .3), -->
<!--                         n = 60000){ -->

<!--   data <- simulate_data(model,  -->
<!--                         n = n, -->
<!--                         parameters = par,  -->
<!--                         using = "parameters")  -->

<!--   m1 <- with(data, c(mean(M[X==0]), mean(M[X==1]))) -->
<!--   m1[2] - m1[1]; m1[2] - 1 + m1[1] -->
<!--   m2<- with(data, c(mean(Y[M==0]), mean(Y[M==1]))) -->
<!--   m2[2] - m2[1]; m2[2] - 1 + m2[1] -->

<!--   if(!exists("fit")) fit <- fitted_model() -->

<!--   updated <- CausalQueries(model, data, stan_model = fit) -->

<!--   # check <- rstan::extract(updated$posterior, pars= "lambdas")$lambdas -->

<!--   phi_b_num <- query_distribution(updated, query = "(M==1) & (Y[X=0]==0)", subset = "X==1 & Y==1", using = "posteriors") -->
<!--   phi_b_denom <- query_distribution(updated, query = "(Y[X=0]==0)", subset = "X==1 & Y==1", using = "posteriors") -->
<!--   phi_b <- phi_b_num/phi_b_denom -->

<!--   phi_d_num <- query_distribution(updated, query = "(M==1) & (Y[X=0]==1)", subset = "X==1 & Y==1", using = "posteriors") -->
<!--   phi_d_denom <- query_distribution(updated, query = "(Y[X=0]==1)", subset = "X==1 & Y==1", using = "posteriors") -->
<!--   phi_d <- phi_d_num/phi_d_denom -->

<!--   out <- data.frame(phi_b, phi_d) -->

<!--   out -->
<!--   } -->

<!-- plot_phi <- function(out, main = "bounds"){ -->
<!--     plot(out$phi_d, out$phi_b, xlim = c(0,1), ylim = c(0,1), cex = .5, main = main,  -->
<!--        xlab = expression(phi[d]), ylab = expression(phi[b])) -->
<!--   abline(0,1) -->
<!-- } -->
<!-- ``` -->

<!-- ```{r writephis, include = FALSE} -->
<!-- if(do_diagnosis){ -->
<!--   phis1  <- make_phis(model = make_model("X -> M -> Y"),  -->
<!--                         par = c(.5, .5, .3, 0, .6, .1, .1, 0, .6, .3), -->
<!--                         n = 80000) -->
<!--   write_rds(phis1, "saved/phis_1.rds") -->

<!--   phis2  <- make_phis(model = make_model("X -> M -> Y"),  -->
<!--                         par = c(.5, .5,  -->
<!--                                 .95, 0, 0, .05,  -->
<!--                                 .95, 0, 0, .05), -->
<!--                         n = 80000) -->
<!--   write_rds(phis2, "saved/phis_2.rds") -->

<!-- } -->
<!-- ``` -->

<!-- ```{r plotphis, echo = FALSE} -->
<!-- phis1 <- read_rds("saved/phis_1.rds") -->
<!-- phis2 <- read_rds("saved/phis_2.rds") -->

<!-- par(mfrow = c(1,2)) -->

<!-- plot_phi(phis1, -->
<!--          main = expression(paste("Hoop: ", tau[1], "= .6, ", -->
<!--                                            rho[1], "= -.2, ", -->
<!--                                            tau[2], "= .6, ", -->
<!--                                            rho[2], "= .2")) -->
<!--          ) -->

<!-- plot_phi(phis2, -->
<!--           main = expression(paste("Smoking gun: ",  -->
<!--                                   tau[1], "= 0, ", -->
<!--                                   rho[1], "= -.9, ", -->
<!--                                   tau[2], "= 0, ", -->
<!--                                   rho[2], "= -.9")) -->
<!--          ) -->

<!-- ``` -->


<!-- <!-- plot_bounds(.6, -.2, .6, .2, 20, main = expression(paste("Hoop: ", tau[1], "= .6, ", --> -->
<!-- <!--                                                             rho[1], "= -.2, ", --> -->
<!-- <!--                                                             tau[2], "= .6, ", --> -->
<!-- <!--                                                             rho[2], "= .2"))) --> -->

<!-- <!-- plot_bounds(0.0, -.9, .0, -.9, 100, main = expression(paste("Smoking gun: ", tau[1], "= 0, ", --> -->
<!-- <!--                                                             rho[1], "= -.9, ", --> -->
<!-- <!--                                                             tau[2], "= 0, ", --> -->
<!-- <!--                                                             rho[2], "= -.9, "))) --> -->

<!-- <!-- ``` --> -->


<!-- <!-- ```{r, eval = FALSE, echo = FALSE} --> -->
<!-- <!-- # An odd one! --> -->
<!-- <!-- plot_bounds(.02, -.5, -.10, -.5,600) --> -->
<!-- <!-- ``` --> -->

<!-- For the smoking gun,  $\phi_{b1}$ is .5 because $\lambda_a^j = \lambda_b^j$ so half of the upper level $b$ types work through a positive effect on $M$ and half through a negative effect on $M$. $\phi_{d1}$, on the other hand, is low here $d$ types mostly arise because of $c$ types in the first step and $a$ types in the second, and hence most commonly with $M=1$.  -->

<!-- Whether the bounds map into useful probative value depends in part on whether causal effects are better identified in the first or the second stage. We can see this in Figure \@ref(fig:probval2). -->

<!-- The key difference between the panels is that $\phi_d$ is constrained to be low in the first panel but not in the second.  -->

<!-- For intuition note that a higher level $d$ type will exhibit $M=1$ if it is formed via $db$, $bd$,or $dd$ and it will exhibit $M=0$ if it is formed via $ca$, $cd$, $ad$. The weak second stage makes it possible that there are no second stage d types, only a and b types. The stronger first stage makes it possible that there are no first stage $c$ types. In that case the higher level d types are formed uniquely of $db$ types -- which always exhibit $M=1$ if $X=1$. -->

<!-- This is not possible however for the data assume in the first panel. In the first panel the the higher value on $\rho_2$ means that there must be at least .25 d types. And the weak first stage means that there must at least .5 a and c types combined. Thus there *must* be a set of cases in which $M$ is not observed even though we have an upper level d type. -->

<!-- ```{r probval2, echo = FALSE, fig.width = 10, fig.cap = "Probative value with different first and second stage relations"} -->

<!-- # par(mfrow = c(1,2)) -->
<!-- #  -->
<!-- # plot_bounds(0, 0, .25, .25, 20, main = expression(paste("Weak first stage: ", tau[1], "= 0, ", -->
<!-- #                                                             rho[1], "= 0, ", -->
<!-- #                                                             tau[2], "= .25 ", -->
<!-- #                                                             rho[2], "= .25"))) -->
<!-- #  -->
<!-- # plot_bounds(.25, .25, 0, 0, 20, main = expression(paste("Weak second stage: ", tau[1], "= .25, ", -->
<!-- #                                                             rho[1], "= .25, ", -->
<!-- #                                                             tau[2], "= 0, ", -->
<!-- #                                                            rho[2], "= 0"))) -->

<!-- if(do_diagnosis){ -->
<!--   phis3  <- make_phis(model = make_model("X -> M -> Y"),  -->
<!--                         par = c(.5, .5,  -->
<!--                                 .25, .25, .25, .25,  -->
<!--                                 .25, 0, .25, .5), -->
<!--                         n = 50000) -->
<!--   write_rds(phis3, "saved/phis_3.rds") -->

<!--   phis4  <- make_phis(model = make_model("X -> M -> Y"),  -->
<!--                         par = c(.5, .5,  -->
<!--                                 .25, 0, .25, .5, -->
<!--                                 .25, .25, .25, .25), -->
<!--                         n = 50000) -->
<!--   write_rds(phis4, "saved/phis_4.rds") -->

<!-- } -->

<!-- phis3 <- read_rds("saved/phis_3.rds") -->
<!-- phis4 <- read_rds("saved/phis_4.rds") -->

<!-- par(mfrow = c(1,2)) -->

<!-- plot_phi(phis3, -->
<!--          main = expression(paste("Hoop: ", tau[1], "= 0, ", -->
<!--                                                             rho[1], "= 0, ", -->
<!--                                                             tau[2], "= .25, ", -->
<!--                                                             rho[2], "= .25"))) -->

<!-- plot_phi(phis4, -->
<!--           main = expression(paste("Smoking gun: ",  -->
<!--                                   tau[1], "= .25, ", -->
<!--                                   rho[1], "= .25, ", -->
<!--                                   tau[2], "= 0, ", -->
<!--                                   rho[2], "= 0, "))) -->

<!-- ``` -->


<!-- In short we emphasize that difficult as it might seem at first it is possible to put relatively tight bounds on probative value for causal types with access to experimental data on exchangeable units.  -->

## Justification from experimental designs

idea: show inferences given for example parallel designs for mediation

### Mediator

Say now that *in addition* we know from experimental data, that $K$ mediates the relationship between $X$ and $Y$; indeed we will assume that we have a case of complete mediation, such that, conditional on $K$, $Y$ does not depend on $X$. 

Say the transition matrices from $X$ to $K$ and $K$ to $Y$ are:

$$P^{xk}=\left( \begin{array}{cc} 1 & 0 \\ 1/2 & 1/2\end{array}\right), P^{ky}=\left( \begin{array}{cc} 1/2 & 1/2 \\ 0 & 1\end{array}\right)$$ 
Even without observing $K$, this information is sufficient to place a prior on PC of $p=\frac13$. 

To see this, note that we can calculate:

* $\lambda_a^K =0$, $\lambda_b^K = \frac{1}{2}$, $\lambda_c^K = \frac{1}{2}$, $\lambda_d^K = 0$
* $\lambda_a^Y =0$, $\lambda_b^Y=\frac{1}{2}$, $\lambda_c^Y=0$,  $\lambda_d^Y=\frac{1}{2}$

and so:

* $\lambda_b^u = \lambda_b^K\lambda_b^Y = \frac{1}4$
* $\lambda_d^u = \lambda_d^Y$ 
* $p = \frac{\lambda_b^u}{\lambda_b^u + \lambda_d^u} = \frac{1}3$.

whence:

* $\phi_{b1} = 1$
* $\phi_{d1} = \lambda_d^K + \lambda_b^K = \frac{1}{2}$

More generally we can calculate the lower bound on the probability that $X$ caused $Y$ as the product of the lower bounds that $X$ caused $M$ and that $M$ caused $Y$, and similarly for the upper bound, using the same formula as before. Signing things so that $\tau^j\geq 0$, $j \in {1,2}$:

$$\frac{2\tau_1}{1+\tau_1+\rho_1}\frac{2\tau_2}{1+\tau_2+\rho_2}  \leq PC \leq \frac{1+\tau_1-|\rho_1|}{1+\tau_1+\rho_1}\frac{1+\tau_2-|\rho_2|}{1+\tau_2+\rho_2} $$


We have undertaken essentially the same operations as above except that now we are placing bounds on a substantive estimand of interest rather than first placing bounds on probative value of a clue and then turning to Bayes rule to place bounds on the estimand.


### Moderator
Consider now  a situation  in which our case is drawn from a set of cases for which $X$ and $K$ were each randomly assigned. Say then that the transition matrices, conditional on $K$ look as follows:

$$P^{K=0}=\left( \begin{array}{cc} 0 & 1 \\ 0.5 & 0.5 \end{array}\right), P^{K=1}=\left( \begin{array}{cc} 1 & 0 \\ 0 & 1 \end{array}\right)$$
In this case we can now identify PC, even before observing $K$. If $K=0$, PC is 0---there are no cases with positive effects in this condition. If $K=1$ PC = 1.  We have a prior  that $K=1$ of  .5 and after observing $X=Y=1$ we raise this to $2/3$. Thus our prior belief on $PC$ --- before seeing $K$--- is $2/3 * 1 + 1/3 * 0 = 2/3$. 

How about $\phi_{b1}$ and $\phi_{d1}$?

Here positive effects only arise when $K=1$ and so $\phi_{b1} = 1$. $Y=1$ without being cause by $X$ only if $K=0$ and so  $\phi_{b0} = 0$. Thus we have a double decisive clue.




## Causal discovery

```{r skeleton, include = FALSE, eval = FALSE}
# source("http://bioconductor.org/biocLite.R")
# biocLite("RBGL")
# if (!requireNamespace("BiocManager", quietly = TRUE))
#     install.packages("BiocManager")
# BiocManager::install("Rgraphviz")
# library(Rgraphviz)

library(CausalQueries)
library(dplyr)
library(Rgraphviz)
library(pcalg)

recover_model <- function(model) {
    
  df <- simulate_data(model, n = 1000)
  V  <- colnames(df)
  
  suffStat <- list(dm = df, nlev = c(2,2,2), adaptDF = FALSE)
  skel.fit <- skeleton(suffStat,
                       indepTest = disCItest, 
                       alpha = 0.01, labels = V, 
                       verbose = TRUE)
  plot(skel.fit, main = "Estimated Skeleton")
  }
```

We start with a model with three variables, $X,M,Y$ where $X$ affects $Y$ directly and indirectly through $M$. We simulate data from this model -- assuming monotonicity but otherwise a flat distribution on types, and then try to recover the structure from this model.



```{r model1ch14, include = FALSE}
model1 <- make_model("X -> M -> Y <- X") %>% 
  set_restrictions(c("(M[X=1]<M[X=0])", 
                     "(Y[M=1, X=.]<Y[M=0, X=.]) | (Y[X=1, M=.]<Y[X=0, M=.])"))
```



In this case the data structure did not impose restrictions on the skeleton. The true graph can however be recovered with knowledge of the temporal ordering of variables. 

Next we consider the model in which X causes Y through M but not directly. In this case we have a restriction --- specifically there is no arrow pointing directly from $X$ to $Y$. Again we impose monotonicity, draw data, and try to recover the model:


```{r model2ch14, include = FALSE}
model2 <- make_model("X -> M -> Y") %>% 
  set_restrictions(c("(M[X=1]<M[X=0])", "(Y[M=1]<Y[M=0])"))
```

Again we have the correct skeleton and knowledge of timing is enough to recover the graph.

Finally we consider the model in which $Y$ has two causes that do not influence each other. Again we impose monotonicity, draw data, and try to recover the model:


```{r model3ch14, include = FALSE}
model3 <- make_model("X1 -> Y <- X2") %>% 
  set_restrictions("(Y[X1=1]<Y[X1=0]) | (Y[X2=1]<Y[X2=0])")
```


```{r plots, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = "DAGs from Data", fig.width = 5, fig.height = 10,results='hide',fig.keep='all', eval = FALSE}
par(mfrow = c(2,3))
par(mar = rep(3, 4))
plot(model1); title("True model")
recover_model(model1)
plot(model2); title("True model")
recover_model(model2)
plot(model3); title("True model")
recover_model(model3)
```


### A model of models

In the following mode there is an unknown, $Q$, which determines the relevant causal model. 
If $Q=1$ then we have $A \rightarrow B \rightarrow C2$; if $Q=0$ then $A \rightarrow B \leftarrow C1$. In this case the temporal order of $C1$ and $C2$ is observable, so there is not confusion there; what is not clear however is which is the important node to include in the model. 

```{r ch14Qornot}

model <- make_model("A -> B -> C2 <- C1 -> B <- Q -> C2") %>%
  # These restrictions capture the role of Q in turning parentage on or off 
  set_restrictions(c(
    "(B[C1=1, Q=1] != B[C1=0, Q=1])",
    "(C2[B=1, Q=0] != C2[B=0, Q=0])")) %>%
  
  # These restrictions are for simplification: monotonicity and complementarity 
  set_restrictions(c(
    "(C2[B=1] < C2[B=0])",
    "(C2[C1=1] < C2[C1=0])",
    "(B[A=1]  < B[A=0])",
    "(B[C1=1] < B[C1=0])",
    "((B[A=1, C1=1] - B[A=0, C1=1]) < (B[A=1, C1=0] - B[A=0, C1=0]))"))
                   
model
                   
plot(model)

if(do_diagnosis){
  model_Q0 <- set_parameters(model, node = "Q", alphas = c(1,0))
  model_Q1 <- set_parameters(model, node = "Q", alphas = c(0,1))
  
  data_0 <- make_data(model_Q0, 100, vars = c("A", "B", "C1", "C2"))
  data_1 <- make_data(model_Q1, 100, vars = c("A", "B", "C1", "C2"))
  
  model_Q0 <- update_model(model, data_0)
  model_Q1 <- update_model(model, data_1)

Qu0 <- query_model(model_Q0, "Q==1", using = "posteriors")
Qu1 <- query_model(model_Q1, "Q==1", using = "posteriors")

  write_rds(list(model_Q0, model_Q1, Qu0, Qu1), "saved/ch14_Qornot.rds")
}

Qornot <- read_rds("saved/ch14_Qornot.rds")

kable(Qornot[[3]])
kable(Qornot[[4]])




```

[Ideally here however $\lambda^Q$ is either 0 or 1 --- we want to know which world we are in]


